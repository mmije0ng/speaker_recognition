import os
import torch
from pydub import AudioSegment
from multiprocessing import Pool
import tempfile
from pyannote.audio import Pipeline, Inference
from pyannote.core import Segment
import torch.nn.functional as F

HF_TOKEN = os.getenv("HF_TOKEN")  # í™˜ê²½ë³€ìˆ˜ì— Hugging Face Token í•„ìš”

'''
WAV ì˜¤ë””ì˜¤ íŒŒì¼ì„ 60ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ 

ê° ì¡°ê°ì— ëŒ€í•´ **í™”ì ë¶„í• (Speaker Diarization)**ì„ ìˆ˜í–‰í•œ í›„

ë¶„í• ëœ ë°œí™” êµ¬ê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³ 

ê°€ì¥ ë¨¼ì € ë“±ì¥í•œ í™”ì êµ¬ê°„ì„ ê¸°ì¤€(reference)ìœ¼ë¡œ ì‚¼ì•„

ë‚˜ë¨¸ì§€ ë°œí™” êµ¬ê°„ë“¤ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬

ê° í™”ìê°€ ê¸°ì¤€ í™”ìì™€ ë™ì¼ì¸ì¸ì§€ ì—¬ë¶€ë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„.
'''

# ì˜¤ë””ì˜¤ ë¶„í•  í•¨ìˆ˜
def split_audio(file_path, chunk_length_ms=60000):
    audio = AudioSegment.from_wav(file_path)
    chunk_paths = []
    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        chunk_path = os.path.join(tempfile.gettempdir(), f"chunk_{i}.wav")
        chunk.export(chunk_path, format="wav")
        chunk_paths.append((chunk_path, i / 1000))  # ì´ˆ ë‹¨ìœ„ ì‹œì‘ ì‹œê°„
    return chunk_paths

# ë³‘ë ¬ diarization í•¨ìˆ˜
def diarize_chunk(args):
    chunk_path, offset = args
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HF_TOKEN)
    result = pipeline(chunk_path)
    return [
        (chunk_path, turn.start + offset, turn.end + offset, speaker)
        for turn, _, speaker in result.itertracks(yield_label=True)
    ]

# ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
def extract_embedding(model, audio_path, start, end):
    return model.crop(audio_path, Segment(start, end))  # torch.tensor([1, 192])

# ìœ ì‚¬ë„ ê³„ì‚°
def verify_speakers(ref_emb, test_emb):
    # numpy â†’ tensor ë³€í™˜
    if not isinstance(ref_emb, torch.Tensor):
        ref_emb = torch.tensor(ref_emb, dtype=torch.float32)
    if not isinstance(test_emb, torch.Tensor):
        test_emb = torch.tensor(test_emb, dtype=torch.float32)

    # (192,) â†’ (1, 192) ë¡œ reshape
    if ref_emb.ndim == 1:
        ref_emb = ref_emb.unsqueeze(0)
    if test_emb.ndim == 1:
        test_emb = test_emb.unsqueeze(0)

    similarity = F.cosine_similarity(ref_emb, test_emb, dim=1).item()
    return similarity


# ì „ì²´ ì‹¤í–‰
def process_with_self_reference(audio_file):
    print("GPU ì‚¬ìš© ì—¬ë¶€:", torch.cuda.is_available())

    # 1. ë¶„í• 
    chunks = split_audio(audio_file, chunk_length_ms=60000)

    # 2. diarization
    with Pool(processes=min(os.cpu_count(), len(chunks))) as pool:
        diarization_results = pool.map(diarize_chunk, chunks)

    all_segments = [seg for chunk in diarization_results for seg in chunk]
    all_segments.sort(key=lambda x: x[1])  # ì‹œì‘ì‹œê°„ ê¸°ì¤€ ì •ë ¬

    if not all_segments:
        print("ë¶„í• ëœ í™”ì êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\n=== í™”ì ë¶„í•  ê²°ê³¼ ===")
    for _, start, end, speaker in all_segments:
        print(f"{start:.1f}s ~ {end:.1f}s : Speaker {speaker}")

    # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = Inference("pyannote/embedding", window="whole")

    # 4. ì²« í™”ì ì„¸ê·¸ë¨¼íŠ¸ë¥¼ referenceë¡œ ì‚¬ìš©
    ref_path, ref_start, ref_end, ref_speaker = all_segments[0]
    print(f"\nğŸ” ê¸°ì¤€ í™”ì: Speaker {ref_speaker}, êµ¬ê°„: {ref_start:.1f}~{ref_end:.1f}s")

    ref_embedding = extract_embedding(embedding_model, ref_path, ref_start, ref_end)

    # 5. ë‚˜ë¨¸ì§€ì™€ ë¹„êµ
    print("\n=== ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ (ê¸°ì¤€ í™”ìì™€ì˜ ë¹„êµ) ===")
    for chunk_path, start, end, speaker in all_segments:
        test_embedding = extract_embedding(embedding_model, chunk_path, start, end)
        similarity = verify_speakers(ref_embedding, test_embedding)

        print(f"[{start:.1f}s~{end:.1f}s] Speaker {speaker} â†’ ìœ ì‚¬ë„: {similarity:.4f}")


# ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    input_audio_path = "data/tfile.wav" # ë¶„ì„í•  ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
    process_with_self_reference(input_audio_path)
