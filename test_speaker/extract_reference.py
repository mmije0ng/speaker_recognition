import os
import numpy as np
import torch
from pydub import AudioSegment
from multiprocessing import Pool
import tempfile
from pyannote.audio import Pipeline
from resemblyzer import VoiceEncoder, preprocess_wav
from collections import defaultdict

SAMPLE_RATE = 16000
HF_TOKEN = os.getenv("HF_TOKEN")

# ğŸ”¹ 1. ì˜¤ë””ì˜¤ ë¶„í• 
def split_audio(file_path, chunk_length_ms=60000):
    audio = AudioSegment.from_wav(file_path)
    chunk_paths = []
    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        chunk_path = os.path.join(tempfile.gettempdir(), f"chunk_{i}.wav")
        chunk.export(chunk_path, format="wav")
        chunk_paths.append((chunk_path, i / 1000))  # ì´ˆ ë‹¨ìœ„
    return chunk_paths, len(audio) / 1000

# ğŸ”¹ 2. ë³‘ë ¬ diarization
def diarize_chunk(args):
    chunk_path, offset = args
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HF_TOKEN)
    result = pipeline(chunk_path)
    return [
        (turn.start + offset, turn.end + offset, speaker)
        for turn, _, speaker in result.itertracks(yield_label=True)
    ]

# ğŸ”¹ 3. ì „ì²´ íë¦„
def diarize_and_extract_embeddings(audio_path, save_path="data/reference_multi.npy"):
    print("GPU ì‚¬ìš© ì—¬ë¶€:", torch.cuda.is_available())

    # 1. ë¶„í• 
    chunks, total_duration = split_audio(audio_path)

    # 2. ë³‘ë ¬ diarization ìˆ˜í–‰
    with Pool(processes=min(os.cpu_count(), len(chunks))) as pool:
        diarization_results = pool.map(diarize_chunk, chunks)

    # 3. ê²°ê³¼ ì •ë¦¬
    all_segments = [seg for result in diarization_results for seg in result]
    all_segments.sort(key=lambda x: x[0])  # ì‹œì‘ ì‹œê°„ ê¸°ì¤€

    print("\n=== í™”ì ë¶„í•  ê²°ê³¼ ===")
    for start, end, speaker in all_segments:
        print(f"{start:.1f}s ~ {end:.1f}s : Speaker {speaker}")

    # 4. í™”ìë³„ ìŒì„± í•©ì¹˜ê¸°
    speaker_segments = defaultdict(list)
    for start, end, speaker in all_segments:
        speaker_segments[speaker].append((start, end))

    audio = AudioSegment.from_wav(audio_path)
    encoder = VoiceEncoder()
    speaker_embeddings = {}

    for speaker, segments in speaker_segments.items():
        speaker_audio = AudioSegment.silent(duration=0)
        for start, end in segments:
            segment = audio[start * 1000:end * 1000]
            speaker_audio += segment

        samples = np.array(speaker_audio.get_array_of_samples()).astype(np.float32) / 32768.0
        if speaker_audio.channels == 2:
            samples = samples.reshape((-1, 2)).mean(axis=1)

        embedding = encoder.embed_utterance(preprocess_wav(samples, SAMPLE_RATE))
        speaker_embeddings[speaker] = embedding
        print(f"âœ… Speaker {speaker} ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ")

    # 5. ì €ì¥
    np.save(save_path, speaker_embeddings)
    print(f"\nğŸ’¾ ëª¨ë“  í™”ì ì„ë² ë”©ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ğŸ”¹ ì‹¤í–‰
if __name__ == "__main__":
    diarize_and_extract_embeddings("data/tfile.wav")
