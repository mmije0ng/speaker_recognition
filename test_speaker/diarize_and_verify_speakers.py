import os
import time
import torch
from pydub import AudioSegment
from multiprocessing import Pool
import tempfile
from pyannote.audio import Pipeline
from pyannote.core import Segment
from resemblyzer import VoiceEncoder, preprocess_wav
from scipy.spatial.distance import cosine
import numpy as np

'''
í™”ì ë¶„í• ì€ ê·¸ëŒ€ë¡œ pyannote.audioì˜ Pipeline ì‚¬ìš©.

ì„ë² ë”© ì¶”ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚°ì€ Resemblyzer ì‚¬ìš© (VoiceEncoder.embed_utterance)

Segment ì‹œê°„ ë²”ìœ„ë¥¼ ì´ìš©í•´ í•´ë‹¹ êµ¬ê°„ë§Œ ì˜ë¼ Resemblyzerì— ì „ë‹¬.
'''

HF_TOKEN = os.getenv("HF_TOKEN")

# ì˜¤ë””ì˜¤ ë¶„í•  í•¨ìˆ˜
def split_audio(file_path, chunk_length_ms=60000):
    audio = AudioSegment.from_wav(file_path)
    chunk_paths = []
    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        chunk_path = os.path.join(tempfile.gettempdir(), f"chunk_{i}.wav")
        chunk.export(chunk_path, format="wav")
        chunk_paths.append((chunk_path, i / 1000))  # ì´ˆ ë‹¨ìœ„ offset
    return chunk_paths

# diarization ìˆ˜í–‰ í•¨ìˆ˜
def diarize_chunk(args):
    chunk_path, offset = args
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HF_TOKEN)
    result = pipeline(chunk_path)
    return [
        (chunk_path, turn.start + offset, turn.end + offset, speaker)
        for turn, _, speaker in result.itertracks(yield_label=True)
    ]

# Resemblyzer ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
def extract_embedding_resemblyzer(encoder, audio_path, start, end):
    # 1. ì „ì²´ ì˜¤ë””ì˜¤ ë¶ˆëŸ¬ì˜¤ê¸°
    audio = AudioSegment.from_wav(audio_path)

    # 2. í•„ìš”í•œ êµ¬ê°„ë§Œ ìë¥´ê¸° (ms ë‹¨ìœ„)
    segment_audio = audio[start * 1000 : end * 1000]

    # 3. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_path = os.path.join(tempfile.gettempdir(), f"clip_{start:.2f}_{end:.2f}.wav")
    segment_audio.export(temp_path, format="wav")

    # 4. preprocess ë° ì„ë² ë”© ì¶”ì¶œ
    wav = preprocess_wav(temp_path)
    return encoder.embed_utterance(wav)

# ìœ ì‚¬ë„ ê³„ì‚° (ResemblyzerëŠ” cosineìœ¼ë¡œ ê³„ì‚°)
def verify_speakers(reference_embedding, test_embedding):
    similarity = 1 - cosine(reference_embedding, test_embedding)
    return similarity

# ì „ì²´ ì‹¤í–‰
def process_with_self_reference(audio_file):
    print("GPU ì‚¬ìš© ì—¬ë¶€:", torch.cuda.is_available())
    total_start = time.time()  # ì „ì²´ ì‹œê°„ ì¸¡ì • ì‹œì‘

    # 1. ì˜¤ë””ì˜¤ ë¶„í• 
    start = time.time()
    chunks = split_audio(audio_file, chunk_length_ms=60000)
    print(f"ğŸ§© ì˜¤ë””ì˜¤ ë¶„í•  ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

    # 2. diarization
    start = time.time()
    with Pool(processes=min(os.cpu_count(), len(chunks))) as pool:
        diarization_results = pool.map(diarize_chunk, chunks)
    print(f"ğŸ”Š í™”ì ë¶„í•  ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

    all_segments = [seg for chunk in diarization_results for seg in chunk]
    all_segments.sort(key=lambda x: x[1])

    if not all_segments:
        print("ë¶„í• ëœ í™”ì êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\n=== í™”ì ë¶„í•  ê²°ê³¼ ===")
    for _, start_time, end_time, speaker in all_segments:
        print(f"{start_time:.1f}s ~ {end_time:.1f}s : Speaker {speaker}")

    # 3. Resemblyzer ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    encoder = VoiceEncoder()

    # 4. ê¸°ì¤€ í™”ì ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
    ref_path, ref_start, ref_end, ref_speaker = all_segments[0]
    print(f"\nğŸ” ê¸°ì¤€ í™”ì: Speaker {ref_speaker}, êµ¬ê°„: {ref_start:.1f}~{ref_end:.1f}s")

    start = time.time()
    reference_embedding = extract_embedding_resemblyzer(encoder, ref_path, ref_start, ref_end)
    print(f"ğŸ™ï¸ ê¸°ì¤€ í™”ì ì„ë² ë”© ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

    # 5. ë‹¤ë¥¸ í™”ìë“¤ê³¼ ë¹„êµ
    print("\n=== ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ (ê¸°ì¤€ í™”ìì™€ì˜ ë¹„êµ) ===")
    start = time.time()
    for chunk_path, start_time, end_time, speaker in all_segments:
        test_embedding = extract_embedding_resemblyzer(encoder, chunk_path, start_time, end_time)
        similarity = verify_speakers(reference_embedding, test_embedding)
        print(f"[{start_time:.1f}s~{end_time:.1f}s] Speaker {speaker} â†’ ìœ ì‚¬ë„: {similarity:.4f}")

        if similarity >= 0.75:
            print("  âœ… ë™ì¼ í™”ìë¡œ ì¶”ì •ë¨")
        else:
            print("  âŒ ë‹¤ë¥¸ í™”ìë¡œ ì¶”ì •ë¨")
    print(f"ğŸ§  í™”ì ë¹„êµ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

    print(f"\nâ±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ")

# ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    input_audio_path = "data/tfile.wav"  # ë¶„ì„í•  ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
    process_with_self_reference(input_audio_path)
