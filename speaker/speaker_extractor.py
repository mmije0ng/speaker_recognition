import os
import numpy as np
import torch
from pydub import AudioSegment
from pyannote.audio import Pipeline
from resemblyzer import VoiceEncoder, preprocess_wav
from collections import defaultdict
from dotenv import load_dotenv

load_dotenv()

# ì„¤ì •
AUDIO_PATH = "data/tfile.wav"
OUTPUT_DIR = "data/speakers_wav"
EMBEDDING_PATH = "data/reference_multi.npy"
HF_TOKEN = os.getenv("HF_TOKEN")
SAMPLE_RATE = 16000

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# GPU ì‚¬ìš© ì—¬ë¶€ ì¶œë ¥
print("GPU ì‚¬ìš© ì—¬ë¶€:", torch.cuda.is_available())

# ì›ë³¸ ì˜¤ë””ì˜¤ ë¡œë“œ
audio = AudioSegment.from_wav(AUDIO_PATH)

# diarization ìˆ˜í–‰
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HF_TOKEN)
diarization = pipeline(AUDIO_PATH)

# í™”ìë³„ êµ¬ê°„ ì •ë¦¬
speaker_segments = defaultdict(list)
for turn, _, speaker in diarization.itertracks(yield_label=True):
    speaker_segments[speaker].append((turn.start, turn.end))

# resemblyzer ì¸ì½”ë” ì´ˆê¸°í™”
encoder = VoiceEncoder()
speaker_embeddings = {}

# ê¸°ì¡´ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸° (ëˆ„ì  ì €ì¥ìš©)
if os.path.exists(EMBEDDING_PATH):
    previous_embeddings = np.load(EMBEDDING_PATH, allow_pickle=True).item()
else:
    previous_embeddings = {}

# ê³ ìœ  speaker index ê³„ì‚°
existing_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith("speaker_") and f.endswith(".wav")]
existing_indices = [int(f.split("_")[1].split(".")[0]) for f in existing_files if f.split("_")[1].split(".")[0].isdigit()]
base_index = max(existing_indices, default=-1) + 1

# í™”ìë³„ ì˜¤ë””ì˜¤ ìƒì„± ë° ì„ë² ë”© ì €ì¥
for i, (speaker, segments) in enumerate(speaker_segments.items()):
    new_speaker_index = base_index + i
    print(f"\nğŸ™ï¸ Speaker {new_speaker_index} ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
    samples_all = []

    for start, end in segments:
        segment = audio[start * 1000:end * 1000]
        samples = np.array(segment.get_array_of_samples()).astype(np.float32) / 32768.0
        if segment.channels == 2:
            samples = samples.reshape((-1, 2)).mean(axis=1)
        samples_all.append(samples)

    full_samples = np.concatenate(samples_all)
    preprocessed = preprocess_wav(full_samples, SAMPLE_RATE)
    embedding = encoder.embed_utterance(preprocessed)

    speaker_key = f"speaker_{new_speaker_index}"
    speaker_embeddings[speaker_key] = embedding
    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {speaker_key}")

    # ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
    speaker_audio = AudioSegment.silent(duration=0)
    for start, end in segments:
        speaker_audio += audio[start * 1000:end * 1000]
    out_path = os.path.join(OUTPUT_DIR, f"{speaker_key}.wav")
    speaker_audio.export(out_path, format="wav")
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {out_path}")

# ì„ë² ë”© ëˆ„ì  ì €ì¥
previous_embeddings.update(speaker_embeddings)
np.save(EMBEDDING_PATH, previous_embeddings)
print(f"\nëª¨ë“  í™”ì ì„ë² ë”©ì´ ëˆ„ì  ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ â†’ '{EMBEDDING_PATH}'")
